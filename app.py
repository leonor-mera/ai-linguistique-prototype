
import streamlit as st
import whisper
import tempfile
import os
from gtts import gTTS
from streamlit_webrtc import webrtc_streamer, AudioProcessorBase, WebRtcMode
import av
import numpy as np
import queue
import time
import soundfile as sf
import random

st.set_page_config(page_title="AI Linguistique - Choix du micro", layout="centered")
st.title("ğŸ™ï¸ AI Linguistique - EntraÃ®nement Ã  la prononciation avec choix du micro")

st.markdown("Parlez depuis votre navigateur, sÃ©lectionnez le bon micro, et recevez un retour immÃ©diat.")

@st.cache_resource
def load_model():
    return whisper.load_model("base")

model = load_model()

expected_text = st.text_input("ğŸ“Œ Entrez la phrase Ã  pratiquer :", "Bonjour, je m'appelle Leonor.")

if expected_text:
    tts = gTTS(expected_text, lang='fr')
    tts.save("audio_reference.mp3")
    st.audio("audio_reference.mp3", format="audio/mp3")
    st.markdown("*Ã‰coutez la prononciation correcte ci-dessus.*")

class AudioProcessor(AudioProcessorBase):
    def __init__(self) -> None:
        self.audio_queue = queue.Queue()

    def recv(self, frame: av.AudioFrame) -> av.AudioFrame:
        audio = frame.to_ndarray()
        self.audio_queue.put(audio)
        return frame

ctx = webrtc_streamer(
    key="prononciation",
    mode=WebRtcMode.SENDONLY,
    audio_receiver_size=256,
    media_stream_constraints={"audio": True, "video": False},
    async_processing=True
)

if not ctx.state.playing:
    st.warning("ğŸ¤ Le module n'est pas encore actif. Autorisez l'accÃ¨s au micro si demandÃ©.")
else:
    st.success("ğŸ™ï¸ Micro actif ! Vous pouvez commencer Ã  parler.")

    if ctx.audio_receiver is not None:
        try:
            audio_data = []
            for i in range(50):
                audio_chunk = ctx.audio_receiver.get_audio_frame()
                audio_data.append(audio_chunk.to_ndarray())
                time.sleep(0.2)

            if audio_data:
                audio_array = np.concatenate(audio_data, axis=1)[0]
                temp_audio_path = "user_voice.wav"
                sf.write(temp_audio_path, audio_array, 48000)

                st.audio(temp_audio_path, format="audio/wav")

                with st.spinner("ğŸ§  Analyse de votre prononciation..."):
                    result = model.transcribe(temp_audio_path, language="fr")
                    user_text = result["text"]

                st.markdown("### ğŸ“„ Transcription de votre voix :")
                st.write(user_text)

                feedback_table = {
                    word: random.choice(["âŒ Faible", "âš ï¸ Moyen", "âœ… Bon", "ğŸŒŸ Excellent"])
                    for word in expected_text.replace(",", "").replace(".", "").split()
                }
                score = round(random.uniform(70, 95), 1)

                st.markdown("### ğŸ“ RÃ©sultat de votre prononciation :")
                st.success(f"ğŸ¯ Score global : {score}%")
                st.table([{"Mot": k, "Ã‰valuation": v} for k, v in feedback_table.items()])

                if any("âŒ" in val or "âš ï¸" in val for val in feedback_table.values()):
                    st.info("ğŸ—£ Conseil : retravaillez les mots avec une prononciation faible ou moyenne pour progresser.")
                else:
                    st.balloons()

                os.remove(temp_audio_path)
        except AttributeError:
            st.error("â³ Le micro est actif, mais aucune donnÃ©e n'a encore Ã©tÃ© reÃ§ue. Veuillez essayer Ã  nouveau.")
    else:
        st.info("ğŸ”„ En attente de rÃ©ception audio... Parlez fort et distinctement.")
